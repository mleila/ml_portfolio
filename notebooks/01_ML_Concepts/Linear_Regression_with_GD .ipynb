{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"../../assets/01_ML_Concepts/LR_GD_1.png\", width=500, height=100></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../../assets/01_ML_Concepts/LR_GD_2.png\", width=500, height=100>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../../assets/01_ML_Concepts/LR_GD_3.png\", width=500, height=100>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement Gradient Descent using the derived gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (100, 3), y shape = (100, 1)\n"
     ]
    }
   ],
   "source": [
    "nb_samples = 100 # n in the whiteboard\n",
    "nb_features = 3 # d in the whiteboard\n",
    "\n",
    "# generate three random vectors \n",
    "x_1 = np.random.normal(0, 0.1, size=nb_samples)\n",
    "x_2 = np.random.normal(0, 0.3, size=nb_samples)\n",
    "x_3 = np.random.normal(0, 0.2, size=nb_samples)\n",
    "\n",
    "# take their linear combination according to vector theta to generate y\n",
    "noise = np.random.normal(0, 0.01, size=nb_samples) # some gaussian noise\n",
    "y = 0.3*x_1 + 0.2*x_2 + 0.1*x_3 + noise # number of parameters corresponidng to nb_features\n",
    "y = y.reshape(-1, 1) # this is important for tf\n",
    "\n",
    "# create the design matrix X\n",
    "X = np.stack([x_1, x_2, x_3], axis=1)\n",
    "\n",
    "# sanity check the dimensions of our matrices\n",
    "print(f'X shape = {X.shape}, y shape = {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, theta, X):\n",
    "    '''\n",
    "    Implement the gradient as shown from the derivation\n",
    "    '''\n",
    "    gradient = (-y.T @ X) + (theta.T @ X.T @ X)\n",
    "    return gradient.T\n",
    "\n",
    "\n",
    "def compute_loss(y_pred, y_true):\n",
    "    '''\n",
    "    Compute the loss function\n",
    "    '''\n",
    "    return np.mean(np.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 loss = 0.09\n",
      "Epoch = 100 loss = 0.00\n",
      "Epoch = 200 loss = 0.00\n",
      "Epoch = 300 loss = 0.00\n",
      "Epoch = 400 loss = 0.00\n",
      "Final Theta: [0.32 0.2  0.09]\n"
     ]
    }
   ],
   "source": [
    "# constants and parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "# initialize theta\n",
    "theta = np.random.normal(0, 1, size=nb_features).reshape(-1, 1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # compute gradients\n",
    "    gradient = compute_gradient(y, theta, X)\n",
    "    \n",
    "    # update variables\n",
    "    theta -= learning_rate * gradient\n",
    "    \n",
    "    # compute current loss\n",
    "    y_pred = X @ theta\n",
    "    loss = compute_loss(y_pred, y)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch = {epoch} loss = {loss:.2f}')\n",
    "        \n",
    "print(f'Final Theta: {theta.squeeze()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, y, model, loss_function):\n",
    "    '''\n",
    "    One Training Epoch\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # apply the model\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # compute current loss\n",
    "        current_loss = loss_function(y_pred, y, model.variables)\n",
    "        \n",
    "    # automatic differentiation step\n",
    "    gradients = tape.gradient(current_loss, model.variables)\n",
    "\n",
    "    # apply gradients\n",
    "    grads_and_vars = zip(gradients, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    return current_loss\n",
    "\n",
    "\n",
    "def train(data, target, model, loss_function, epochs, verbose=True):\n",
    "    '''\n",
    "    Full training pipeline\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        current_loss = train_epoch(data, target, model, loss_function)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, current loss {current_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, current loss 51.57\n",
      "Epoch 100, current loss 0.04\n",
      "Epoch 200, current loss 0.01\n",
      "Epoch 300, current loss 0.01\n",
      "Epoch 400, current loss 0.01\n",
      "Final Theta: [0.3 0.2 0.1]\n"
     ]
    }
   ],
   "source": [
    "# constants and parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "# define the model\n",
    "class LinearModel:\n",
    "    \n",
    "    def __init__(self, nb_features):\n",
    "        self.nb_features = nb_features\n",
    "        self._init_variables()\n",
    "        \n",
    "        \n",
    "    def _init_variables(self):\n",
    "        theta = tf.random.normal(shape=(self.nb_features, 1))\n",
    "        self.theta = tf.Variable(theta, trainable=True, dtype=tf.float32)\n",
    "        self.variables = [self.theta]\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        theta = self.variables[0]\n",
    "        return tf.matmul(X, theta)\n",
    "\n",
    "\n",
    "# define the loss function\n",
    "def unregularized_loss_function(y_pred, y_true, variables=None):\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    sum_squared_error = tf.reduce_sum(squared_error)\n",
    "    return tf.reduce_mean(sum_squared_error)\n",
    "\n",
    "\n",
    "# define the data\n",
    "data = tf.constant(X, dtype=tf.float32)\n",
    "target = tf.constant(y.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# initialize the model\n",
    "model = LinearModel(nb_features)\n",
    "\n",
    "\n",
    "# choose an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "# train\n",
    "train(data, target, model, unregularized_loss_function, epochs, verbose=True)\n",
    "\n",
    "# print results\n",
    "theta_tensor = model.variables[0]\n",
    "print(f'Final Theta: {theta_tensor.numpy().squeeze()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a nice framework written in Tensorflow, let's run some expirements to get a feel of how different loss functions work. The main idea here is to introduce regularization with the $l_1$ and $l_2$ norms (lasso and ridge) and see how this choice impacts the results.\n",
    "\n",
    "First let's show how the $l_1$ norm encourages sparse solutions and can work as a feature selector. I will create a bunch of useless features, then regularize my loss function with the $l_1$ norm to see them gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (500, 20), y shape = (500, 1)\n"
     ]
    }
   ],
   "source": [
    "nb_samples = 500\n",
    "nb_features = 20 # three real and two useless\n",
    "\n",
    "X = np.random.rand(nb_samples, nb_features)\n",
    "\n",
    "noise = np.random.normal(0, 0.1, size=nb_samples)\n",
    "\n",
    "true_theta = np.zeros(shape=(nb_features, 1))\n",
    "true_theta[:10] = 0.5\n",
    "true_theta[10:] = 0.001\n",
    "\n",
    "y = X @ true_theta   #notice how little x_4 and x_5 contribute to y\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# sanity check the dimensions of our matrices\n",
    "print(f'X shape = {X.shape}, y shape = {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, current loss 1704.86\n",
      "Epoch 100, current loss 274.93\n",
      "Epoch 200, current loss 51.32\n",
      "Epoch 300, current loss 10.46\n",
      "Epoch 400, current loss 2.30\n",
      "Epoch 500, current loss 0.54\n",
      "Epoch 600, current loss 0.13\n",
      "====\n",
      "Epoch 0, current loss 6573.56\n",
      "Epoch 100, current loss 160.38\n",
      "Epoch 200, current loss 37.41\n",
      "Epoch 300, current loss 12.30\n",
      "Epoch 400, current loss 6.72\n",
      "Epoch 500, current loss 5.45\n",
      "Epoch 600, current loss 5.13\n",
      "====\n",
      "Epoch 0, current loss 2463.77\n",
      "Epoch 100, current loss 209.32\n",
      "Epoch 200, current loss 42.64\n",
      "Epoch 300, current loss 10.85\n",
      "Epoch 400, current loss 4.34\n",
      "Epoch 500, current loss 2.91\n",
      "Epoch 600, current loss 2.58\n"
     ]
    }
   ],
   "source": [
    "# constants and parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 700\n",
    "\n",
    "\n",
    "# write the l1 regularized loss function\n",
    "def lasso_loss_function(y_pred, y_true, variables, lmbda=1):\n",
    "    theta = variables[0]\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(y_pred - y_true))) + lmbda * tf.reduce_sum(tf.abs(theta))\n",
    "\n",
    "\n",
    "# write the l2 regularized loss function\n",
    "def ridge_loss_function(y_pred, y_true, variables, lmbda=1):\n",
    "    theta = variables[0]\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(y_pred - y_true))) + lmbda * tf.reduce_sum(tf.square(tf.abs(theta)))\n",
    "\n",
    "\n",
    "# define the data\n",
    "data = tf.constant(X, dtype=tf.float32)\n",
    "target = tf.constant(y.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# initialize the model\n",
    "model = LinearModel(nb_features)\n",
    "\n",
    "\n",
    "# choose an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "# train unregularized\n",
    "model = LinearModel(nb_features)\n",
    "train(data, target, model, unregularized_loss_function, epochs, verbose=True)\n",
    "theta_unreg = model.variables[0]\n",
    "print('====')\n",
    "\n",
    "# train lasso\n",
    "model = LinearModel(nb_features)\n",
    "train(data, target, model, lasso_loss_function, epochs, verbose=True)\n",
    "theta_lasso = model.variables[0]\n",
    "print('====')\n",
    "\n",
    "# train ridge\n",
    "model = LinearModel(nb_features)\n",
    "train(data, target, model, ridge_loss_function, epochs, verbose=True)\n",
    "theta_ridge = model.variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Theta (Unregularized): [ 0.49  0.51  0.5   0.49  0.48  0.49  0.5   0.51  0.5   0.49  0.    0.\n",
      "  0.01  0.01  0.    0.01 -0.   -0.    0.01  0.01]\n",
      "Final Theta (Lasso): [0.49 0.5  0.5  0.49 0.49 0.49 0.5  0.51 0.5  0.49 0.   0.01 0.02 0.\n",
      " 0.01 0.   0.   0.   0.   0.  ]\n",
      "Final Theta (Ridge): [0.5  0.49 0.5  0.48 0.49 0.49 0.49 0.48 0.48 0.5  0.01 0.   0.   0.\n",
      " 0.02 0.01 0.   0.01 0.02 0.01]\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "theta_tensor = model.variables[0]\n",
    "print(f'Final Theta (Unregularized): {theta_unreg.numpy().squeeze()}')\n",
    "print(f'Final Theta (Lasso): {theta_lasso.numpy().squeeze()}')\n",
    "print(f'Final Theta (Ridge): {theta_ridge.numpy().squeeze()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when we crank up the regularization parameter $\\lambda$ to very large values, the lasso regularizer keeps the most important features and suppresses the less important ones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
