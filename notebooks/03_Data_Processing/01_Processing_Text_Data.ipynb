{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores basic text processing procedures in python. We will only stick to preprocessing and creating meaningful representations based on existing models, rather than delve into Natural Language Processing discussions. \n",
    "\n",
    "Here is an outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Preprocessing Techniques**\n",
    "\n",
    "1. Tokenization\n",
    "2. Stemming & Lemmatization\n",
    "3. Stop Words\n",
    "4. Removing punctuations\n",
    "\n",
    "**Basic Vector representation**\n",
    "\n",
    "4. Bag of Words\n",
    "5. Term Frequency — Inverse Document Frequency (TF-IDF)\n",
    "6. n-grams (unigrams, bigarams, ...)\n",
    "\n",
    "**Semantic Vecotr representation**\n",
    "\n",
    "7. Word2vec\n",
    "\n",
    "**Popular Libraries**\n",
    "1. Gensim\n",
    "2. nltk\n",
    "3. spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/mohamed/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # Natural Language Toolkit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download()\n",
    "#nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University.\n",
    "This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. See a list [here](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now that he knew himself to be self he was free to grok ever closer to his brothers , merge without let . Self's integrity was and is and ever had been . Mike stopped to cherish all his brother selves , the many threes-fulfilled on Mars , corporate and discorporate , the precious few on Earth -- the unknown powers of three on Earth that would be his to merge with and cherish now that at last long waiting he grokked and cherished himself . Mike remained in trance ; ; there was much to grok , loose ends to puzzle over and fit into his growing -- all that he had seen and heard and been at the Archangel Foster Tabernacle ( not just cusp when he and Digby had come face to face alone ) why Bishop Senator Boone made him warily uneasy , how Miss Dawn Ardent tasted like a water brother when she was not , the smell of goodness he had incompletely grokked in the jumping up and down and wailing -- Jubal's conversations coming and going -- Jubal's words troubled him most ; ; he studied them , compared them with what he\n"
     ]
    }
   ],
   "source": [
    "reader = brown.words(categories='science_fiction')[:200]\n",
    "words_list = list(reader)\n",
    "text = ' '.join(words_list)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now that he knew himself to be self he was free to grok ever closer to his brothers , merge without let .\n",
      "\n",
      "Self's integrity was and is and ever had been .\n",
      "\n",
      "Mike stopped to cherish all his brother selves , the many threes-fulfilled on Mars , corporate and discorporate , the precious few on Earth -- the unknown powers of three on Earth that would be his to merge with and cherish now that at last long waiting he grokked and cherished himself .\n",
      "\n",
      "Mike remained in trance ; ; there was much to grok , loose ends to puzzle over and fit into his growing -- all that he had seen and heard and been at the Archangel Foster Tabernacle ( not just cusp when he and Digby had come face to face alone ) why Bishop Senator Boone made him warily uneasy , how Miss Dawn Ardent tasted like a water brother when she was not , the smell of goodness he had incompletely grokked in the jumping up and down and wailing -- Jubal's conversations coming and going -- Jubal's words troubled him most ; ; he studied them , compared them with what he\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make sentences (each sentence is a token)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now', 'that', 'he', 'knew', 'himself'] ...\n"
     ]
    }
   ],
   "source": [
    "# make words (each word is a token)\n",
    "words = nltk.word_tokenize(text)[:5]\n",
    "print(words, '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be\n",
    "\n",
    "car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time\n",
    "\n",
    "**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "\n",
    "Source: Stanford NLP Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "leav\n",
      "fairli\n"
     ]
    }
   ],
   "source": [
    "# stemming example\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['grows', 'leaves', 'fairly']\n",
    "for word in words:\n",
    "    stemmed_word = ps.stem(word)\n",
    "    print(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grows\n",
      "leaf\n",
      "fairly\n"
     ]
    }
   ],
   "source": [
    "# lemmetization example \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "words = ['grows', 'leaves', 'fairly']\n",
    "for word in words:\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words usually refers to the most common words in a language like of, them , they, or, etc. Words that do not add too much information for our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw words 1000\n",
      "Number of filtered words 639\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "# use the brown scifi corpus data\n",
    "reader = brown.words(categories='science_fiction')[:1000]\n",
    "words_list = list(reader)\n",
    "\n",
    "# filter out stop words\n",
    "filtered_words = [word for word in words_list if word not in stopwords_en]\n",
    "\n",
    "print(f'Number of raw words {len(words_list)}')\n",
    "print(f'Number of filtered words {len(filtered_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now that he knew himself to be self he was free to grok ever closer to his brothers merge without let Self s integrity was and is and ever had been Mike stopped to cherish all his brother selves the many threes fulfilled on Mars corporate and discorporate the precious few on Earth the unknown powers of three on Earth that would be his to merge with and cherish now that at last long waiting he grokked and cherished himself Mike remained in trance there was much to grok loose ends to puzzle over and fit into his growing all that he had seen and heard and been at the Archangel Foster Tabernacle not just cusp when he and Digby had come face to face alone why Bishop Senator Boone made him warily uneasy how Miss Dawn Ardent tasted like a water brother when she was not the smell of goodness he had incompletely grokked in the jumping up and down and wailing Jubal s conversations coming and going Jubal s words troubled him most he studied them compared them with what he'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "clean_words = regex_tokenizer.tokenize(text)\n",
    "\n",
    "' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now that he knew himself to be self he was free to grok ever closer to his brothers merge without let Self integrity was and is and ever had been Mike stopped to cherish all his brother selves the many on Mars corporate and discorporate the precious few on Earth the unknown powers of three on Earth that would be his to merge with and cherish now that at last long waiting he grokked and cherished himself Mike remained in trance there was much to grok loose ends to puzzle over and fit into his growing all that he had seen and heard and been at the Archangel Foster Tabernacle not just cusp when he and Digby had come face to face alone why Bishop Senator Boone made him warily uneasy how Miss Dawn Ardent tasted like a water brother when she was not the smell of goodness he had incompletely grokked in the jumping up and down and wailing Jubal conversations coming and going Jubal words troubled him most he studied them compared them with what he'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words = [word for word in nltk.word_tokenize(text) if word.isalpha()]\n",
    "\n",
    "' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively we can use the isalpha method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "let's get some sentences and try to represent them using the BOW vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knew self free grok ever closer brothers merge without let\n",
      "self integrity ever\n",
      "mike stopped cherish brother selves many mars corporate discorporate precious earth unknown powers three earth would merge cherish last long waiting grokked cherished\n",
      "mike remained trance much grok loose ends puzzle fit growing seen heard archangel foster tabernacle cusp digby come face face alone bishop senator boone made warily uneasy miss dawn ardent tasted like water brother smell goodness incompletely grokked jumping wailing jubal conversations coming going jubal words troubled studied compared taught nestling struggling bridge languages one thought one learning think\n",
      "word church turned among jubal words gave knotty difficulty martian concept match unless one took church worship god congregation many words equated totality world known forced concept back english phrase rejected differently jubal mahmoud digby\n",
      "thou art god\n",
      "closer understanding english although could never inevitability martian concept stood\n",
      "mind spoke simultaneously english sentence martian word felt closer grokking\n",
      "repeating like student telling jewel lotus sank nirvana\n",
      "midnight speeded heart resumed normal breathing ran check list uncurled sat\n",
      "weary felt light gay ready many actions saw spreading\n",
      "felt puppyish need company strong earlier necessity quiet\n",
      "stepped hall delighted encounter water brother\n",
      "hi\n",
      "oh\n",
      "hello mike\n",
      "look chipper\n",
      "feel fine\n",
      "everybody\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "reader = brown.words(categories='science_fiction')[:500]\n",
    "words_list = list(reader)\n",
    "text = ' '.join(words_list)\n",
    "\n",
    "# lowercase only\n",
    "text = text.lower()\n",
    "\n",
    "# words tokenizer\n",
    "words_list = nltk.word_tokenize(text)\n",
    "\n",
    "# remove stop words\n",
    "filtered_words = [word for word in words_list if word not in stopwords_en]\n",
    "text = ' '.join(filtered_words)\n",
    "\n",
    "# get sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# remove punctuations, symbols (!, ?), and letters\n",
    "clean_sentences = []\n",
    "for sent in sentences:\n",
    "    # tokenize words\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    # remove all tokens that are not alphabetic\n",
    "    sent_words = [word for word in sent_words if word.isalpha()]\n",
    "    # join words back together to form sentence\n",
    "    sent = ' '.join(sent_words)\n",
    "    # drop empty sentences\n",
    "    if not sent:\n",
    "        continue\n",
    "    clean_sentences.append(sent)\n",
    "    \n",
    "for sent in clean_sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our BOW vector representation for each each sentence is a vector of dimensions = number of all words, and values 1 or 0 depending on whether the word is mentioned in the sentence at a given frequency threshold\n",
    "\n",
    "example\n",
    "\n",
    "- they wrote some code\n",
    "- code is being written\n",
    "- they like sports\n",
    "\n",
    "removing stop words and lemmatizing we get\n",
    "\n",
    "- write some code\n",
    "- code write\n",
    "- like sport\n",
    "\n",
    "all words:\n",
    "\n",
    "write, some, code, like, sport\n",
    "\n",
    "representation\n",
    "\n",
    "- sent1 : [1, 1, 1, 0, 0] first word write is present last word sport is not present\n",
    "- sent2 : [1, 0, 1, 0, 0]\n",
    "- sent3 : [0, 0, 0, 1, 1]\n",
    "\n",
    "You can see how this could give a rough indication of similarity between vectors representing sentences that look alike. However, the reperesentation is sparse and problematic. For instance, different words (or terms) are all given the same weight, when some can be more meaningful than others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 167 unique words\n",
      "our matrix has 19 rows and 167 cols\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def make_bow(sentences: List, threshold: int=1)-> List[List[int]]:\n",
    "    # unique words\n",
    "    text = ' '.join(sentences)\n",
    "    all_words = text.split(' ')\n",
    "    unique_words = list(set(all_words))\n",
    "    n = len(unique_words)\n",
    "    print(f'We have {n} unique words')\n",
    "\n",
    "    # instantiate vectors\n",
    "    vectors = [[0 for i in range(n)] for _ in range(len(sentences))]\n",
    "\n",
    "    threshold = 1\n",
    "    for sent_index, sent in enumerate(sentences):\n",
    "        \n",
    "        # tokenize words\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        \n",
    "        # compute frequency of each word in the sentence\n",
    "        counter = Counter(words)\n",
    "        \n",
    "        # for every word, check if its freq passes the threshold\n",
    "        # and if it does set the word index in the sentence vector\n",
    "        # to 1\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= threshold:\n",
    "                word_index = unique_words.index(word)\n",
    "                vectors[sent_index][word_index] = 1\n",
    "    return vectors\n",
    "\n",
    "vectors = make_bow(clean_sentences)\n",
    "\n",
    "# sanity check\n",
    "nrows, ncols = np.array(vectors).shape\n",
    "print(f'our matrix has {nrows} rows and {ncols} cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 5 unique words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try our simple example for ease of visualization\n",
    "sentences = ['write some code', 'code write', 'like sport']\n",
    "np.array(make_bow(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to implement this from scratch, we can use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 0, 1],\n",
       "        [1, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=20)\n",
    "matrix = cv.fit_transform(sentences, )\n",
    "matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code', 'like', 'some', 'sport', 'write']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note that the order of columns is different, so we can get the feature names from the count vectorizer\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf–idf is the product of two statistics, term frequency and inverse document frequency. It is intended to reflect how important a word is to a document in a collection or corpus. There are various ways for determining the exact values of both statistics\n",
    "\n",
    "Term Frequency = $\\frac{term\\:count}{number\\,of\\,words\\,in\\,the\\,sentence}$\n",
    "\n",
    "\n",
    "Inverse Document Frequency = $log(\\frac{1 + number\\:of\\:sentences}{1 + number\\,of\\,sentences\\,with\\,this\\,word}) + 1$\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "Example\n",
    "\n",
    "- write some code\n",
    "- write python code\n",
    "- write essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>essay</th>\n",
       "      <th>python</th>\n",
       "      <th>some</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           code     essay    python      some     write\n",
       "sent1  0.547832  0.000000  0.000000  0.720333  0.425441\n",
       "sent2  0.547832  0.000000  0.720333  0.000000  0.425441\n",
       "sent3  0.000000  0.861037  0.000000  0.000000  0.508542"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "sentences = ['write some code', 'write python code', 'write essay']\n",
    "\n",
    "data = tfidf.fit_transform(sentences).todense()\n",
    "words = tfidf.get_feature_names()\n",
    "pd.DataFrame(data, columns=words, index=['sent1', 'sent2', 'sent3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Vector Representations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'write': <gensim.models.keyedvectors.Vocab object at 0x136920390>, 'some': <gensim.models.keyedvectors.Vocab object at 0x136920128>, 'code': <gensim.models.keyedvectors.Vocab object at 0x136920048>, 'like': <gensim.models.keyedvectors.Vocab object at 0x1369200f0>, 'sport': <gensim.models.keyedvectors.Vocab object at 0x1369205c0>}\n",
      "-----\n",
      "Vector Representation of the word \"code\":  [ 0.06649669 -0.0045267   0.10990859  0.07124589]\n",
      "-----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('like', -0.08335989713668823), ('sport', -0.1381823718547821)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "sentences = ['write some code', 'code write', 'like sport']\n",
    "\n",
    "# represent each sentence as a list of words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Apply the model to the sentences (size indicates length of embedding vector)\n",
    "model = Word2Vec(tokenized_sentences, min_count=1, size=4)\n",
    "\n",
    "# access model vocab\n",
    "words = model.wv.vocab\n",
    "print(words)\n",
    "print('-----')\n",
    "\n",
    "# to extract a specific word vector\n",
    "word = 'code'\n",
    "word_vec = model.wv[word]\n",
    "print(f'Vector Representation of the word \"{word}\": ', word_vec)\n",
    "print('-----')\n",
    "\n",
    "# find top similar words to your word\n",
    "model.wv.most_similar(word, topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
